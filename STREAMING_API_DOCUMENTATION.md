# Streaming API Documentation

## Overview

The BB Backend implements a **dual-layer nested streaming architecture** that provides real-time progress updates during AI code generation workflows.

---

## Architecture

### Layer 1: LangGraph Workflow Streaming
- **Purpose:** Stream workflow execution state changes
- **Technology:** LangGraph's built-in `astream()` with dual stream modes
- **Stream Modes:**
  - `updates` - Node state transitions (start, complete, error)
  - `custom` - Custom events from within nodes (tokens, progress messages)

### Layer 2: LLM Token Streaming
- **Purpose:** Stream individual tokens as they're generated by LLM
- **Technology:** LangChain's async streaming with `astream()`
- **Nodes Supporting LLM Streaming:**
  - **Intent Node** - Streams semantic analysis JSON
  - **Codegen Node** - Streams Python code generation
  - **Review Node** - Streams review suggestions (mock)
  - **QA Node** - Streams test case generation (mock)

---

## SSE Events

### Event Types

#### 1. Workflow-Level Events

**`workflow_start`**
- Triggered when workflow begins execution
- Contains basic workflow metadata

**`node_start`**
- Triggered when any workflow node begins execution
- Contains node name and start timestamp

**`node_complete`**
- Triggered when node finishes successfully or with error
- Contains node name, status, duration, and metadata
- Status values: `completed`, `error`

**`node_error`**
- Triggered when node execution fails
- Contains node name, error message, and duration

**`workflow_error`**
- Triggered when entire workflow fails
- Contains error details, retry counts, and execution stats

**`final_result`**
- Contains complete workflow output including generated code, execution results, and file URLs

**`done`**
- Final event signaling stream completion
- Status: `success` or `error`

#### 2. Node-Level Streaming Events

**`llm_token`** (Layer 2 - LLM Streaming)
- Triggered for each token/chunk generated by LLM
- **Key Fields:**
  - `node` - Which node is streaming (intent/codegen/review/qa)
  - `content_type` - Type of content being streamed:
    - `code` - Python code (Codegen Node)
    - `analysis` - Semantic analysis JSON (Intent Node)
    - `suggestion` - Review suggestions (Review Node)
    - `test_case` - Test case code (QA Node)
  - `token` - The actual token/character being streamed
  - `accumulated_length` - Total length of accumulated content

**`node_progress`** (Layer 2 - Custom Events)
- Human-readable progress messages from nodes
- Examples:
  - "Starting intent analysis"
  - "Performing semantic analysis"
  - "Analyzing performance issues"
  - "Code review completed"

#### 3. Lifecycle Events

**`conversation_created`**
- First event after SSE connection established
- Contains conversation ID for tracking

**`files_uploaded`**
- Triggered after user files successfully uploaded
- Contains file count

**`client_disconnected`**
- Server-side detection of client disconnection
- Allows graceful cleanup

---

## Streaming Flow

### Typical Event Sequence

```
1. conversation_created
2. files_uploaded (if files present)
3. workflow_start

4. node_start (intent)
5. node_progress (intent) - "Starting intent analysis"
6. llm_token (intent) - Multiple events streaming JSON
7. node_progress (intent) - "Intent analysis completed"
8. node_complete (intent)

9. node_start (codegen)
10. llm_token (codegen) - Multiple events streaming code
11. node_complete (codegen)

12. node_start (execution)
13. execution_output - Real-time execution output
14. node_complete (execution)

15. node_start (review)
16. node_progress (review) - "Starting code review"
17. llm_token (review) - Streaming suggestions
18. node_progress (review) - "Code review completed"
19. node_complete (review)

20. node_start (qa)
21. llm_token (qa) - Streaming test cases
22. node_complete (qa)

23. final_result
24. done
```

### Error Flow

```
1. conversation_created
2. workflow_start
3. node_start (intent)
4. node_progress (intent) - "Error: ..."
5. node_error (intent)
6. node_complete (intent) - status: error
7. workflow_error
8. done - status: error
```

---

## Dual-Layer Streaming in Detail

### How the Layers Interact

**Layer 1 (Workflow Level):**
- LangGraph manages overall workflow state
- Emits `node_start`, `node_complete` for each node execution
- Tracks retry logic and error propagation
- Provides checkpoint persistence

**Layer 2 (LLM/Node Level):**
- Individual nodes stream their own content
- Uses `get_stream_writer()` to inject custom events into Layer 1
- Bypasses workflow state, directly pushes to SSE
- Real-time granularity (token-by-token)

**Integration Point:**
```
LangGraph.astream(stream_mode=["updates", "custom"])
│
├─ updates → node_start, node_complete (Layer 1)
│
└─ custom → llm_token, node_progress (Layer 2)
```

### Why Dual-Layer?

**Layer 1 Alone:**
- ✓ Tracks workflow progress
- ✗ No visibility into LLM generation process
- ✗ Users see blank screen during long LLM calls

**Layer 2 Added:**
- ✓ Real-time token streaming
- ✓ ChatGPT-like user experience
- ✓ Immediate feedback during generation
- ✓ Better perceived performance

---

## Content Type Differentiation

The `content_type` field in `llm_token` events enables proper frontend rendering:

### `code` (Codegen Node)
- Syntax highlighting for Python
- Monospace font display
- Line number rendering

### `analysis` (Intent Node)
- JSON formatting
- Collapsible sections
- Pretty-printed display

### `suggestion` (Review Node)
- Markdown rendering
- Bullet points for findings
- Color-coded severity levels

### `test_case` (QA Node)
- Python test code highlighting
- Test framework annotations
- Assertion highlighting

---

## Error Handling

### Streaming Timeout
- **Trigger:** LLM streaming exceeds `LLM_TIMEOUT` (default 60s)
- **Behavior:** 
  - Manual timeout check every token
  - Raises `LLMError` with elapsed time
  - Sends `node_progress` with error message
  - Propagates to `workflow_error` event

### Client Disconnection
- **Trigger:** `request.is_disconnected()` returns true
- **Behavior:**
  - Immediately stops workflow iteration
  - Sends `client_disconnected` event
  - Logs disconnection with conversation ID
  - Prevents wasted LLM API calls

### Node-Level Errors
- **Trigger:** Exception during node execution or streaming
- **Behavior:**
  - Sends `node_progress` with error summary (first 100 chars)
  - Sends `node_error` event with full details
  - Marks `node_complete` with `status: error`
  - Workflow retry logic determines next step

---

## Performance Characteristics

### Latency
- **First Token:** ~500ms - 2s (Intent/Codegen nodes)
- **Token Rate:** 20-50 tokens/sec (real LLM streaming)
- **Mock Streaming:** 200 tokens/sec (Review/QA nodes)

### Resource Usage
- **Memory:** Stateful SSE connection (~10KB per connection)
- **CPU:** Minimal (async/await based)
- **Network:** ~100 bytes per event, ~5-10 events/sec during LLM streaming

### Scalability
- **Concurrent Streams:** Limited by Uvicorn workers (default 4)
- **Max Connection Duration:** ~5-60s per request (depends on query complexity)
- **Cleanup:** Automatic on client disconnect or workflow completion

---

## Configuration

### Environment Variables

**`LLM_TIMEOUT`** (default: 60)
- Maximum seconds for LLM streaming
- Applies to both regular and streaming calls

**`USE_MOCK_REVIEW_LLM`** (default: true)
- Enable/disable mock streaming for Review Node
- Set to `false` when real LLM implementation ready

**`USE_MOCK_QA_LLM`** (default: true)
- Enable/disable mock streaming for QA Node
- Set to `false` when real LLM implementation ready

### Mock Streaming Speed

**Review Node:** 5ms per character
- 100 char suggestion = 0.5 seconds

**QA Node:** 20ms per token  
- 100 token test case = 2 seconds

---

## Frontend Integration Considerations

### Event Listener Setup
- Must handle all 13+ event types
- Use event-specific handlers (not generic `message` event)
- Implement proper cleanup on unmount

### State Management
- Accumulate `llm_token` events by node and content_type
- Track node progress separately from streaming content
- Handle out-of-order events gracefully

### UI Rendering
- Use content_type to determine rendering strategy
- Show loading/streaming indicators during `node_progress`
- Display errors prominently from `node_error` events
- Implement optimistic UI updates

### Connection Management
- Auto-reconnect on network failure (with exponential backoff)
- Clear accumulated state on reconnect
- Handle `client_disconnected` gracefully
- Close EventSource on component unmount

---

## Limitations

### Current Limitations
- No stream pause/resume functionality
- No streaming rate limiting per user
- Mock streaming only for Review and QA nodes
- No streaming progress percentage

### Known Issues
- Large token accumulation can cause memory pressure on frontend
- No automatic stream compression (gzip)
- No stream multiplexing (one request = one stream)

---

## Security Considerations

### Authentication
- JWT token required in `Authorization` header
- Validated before streaming begins
- Token refresh not supported during active stream

### Rate Limiting
- Per-user token quota enforced before workflow starts
- No additional rate limiting during streaming
- Client disconnection helps prevent abuse

### Data Privacy
- Streaming content not logged to external services (unless LangSmith enabled)
- SSE connections use HTTPS in production
- No sensitive data in event metadata

---

## Monitoring & Debugging

### Key Log Messages
- "Entering workflow.astream loop" - Workflow streaming started
- "Custom event: llm_token from {node}" - Token streaming detected
- "Client disconnected, stopping stream" - Client disconnect detected
- "Streaming timeout after {elapsed}s" - Timeout protection triggered

### Metrics to Track
- Average tokens per second (by node)
- Stream duration percentiles (p50, p95, p99)
- Client disconnect rate
- Timeout occurrence rate
- Token accumulation size distribution

### Debugging Tips
- Enable `LOG_LEVEL=DEBUG` to see all chunk processing
- Check `node_progress` events for human-readable status
- Monitor `accumulated_length` to verify streaming progress
- Use browser DevTools Network tab to inspect SSE frames

---

## Comparison to Alternatives

### vs. WebSocket
- **SSE Advantages:** Simpler, HTTP-based, auto-reconnect, built-in event types
- **SSE Disadvantages:** Unidirectional only, no binary support

### vs. HTTP Polling
- **Streaming Advantages:** Real-time, lower latency, less bandwidth
- **Streaming Disadvantages:** Connection held open, server resources

### vs. GraphQL Subscriptions  
- **SSE Advantages:** No schema required, simpler backend, standard protocol
- **SSE Disadvantages:** Less structured, no query language

---

## Future Enhancements

### Planned
- Real LLM streaming for Review Node
- Real LLM streaming for QA Node
- Streaming progress percentage calculation
- Configurable streaming speed for mocks

### Under Consideration
- Stream pause/resume via control events
- Per-user streaming rate limits
- Stream compression (gzip/brotli)
- Multi-stream multiplexing
- Streaming analytics dashboard

---

## Summary

The dual-layer streaming architecture provides:

✅ **Real-time visibility** into both workflow execution and LLM generation  
✅ **Fine-grained control** via separate Layer 1 (workflow) and Layer 2 (content) events  
✅ **Content-aware rendering** via `content_type` field  
✅ **Robust error handling** with timeout and disconnect detection  
✅ **Production-ready** with proper logging, monitoring hooks, and security  

This design balances **user experience** (ChatGPT-like streaming), **developer experience** (clear event structure), and **operational excellence** (resource management, error handling).
